{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import tushare as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% of the rows have been processed at 2023-10-26 14:17:10.462358\n",
      "2% of the rows have been processed at 2023-10-26 14:17:18.070459\n",
      "3% of the rows have been processed at 2023-10-26 14:17:33.701373\n",
      "4% of the rows have been processed at 2023-10-26 14:18:04.478746\n",
      "5% of the rows have been processed at 2023-10-26 14:18:44.025202\n",
      "6% of the rows have been processed at 2023-10-26 14:19:40.543353\n",
      "7% of the rows have been processed at 2023-10-26 14:20:50.341588\n",
      "8% of the rows have been processed at 2023-10-26 14:22:09.278401\n",
      "9% of the rows have been processed at 2023-10-26 14:24:03.488094\n",
      "10% of the rows have been processed at 2023-10-26 14:26:19.018052\n",
      "11% of the rows have been processed at 2023-10-26 14:28:35.988304\n",
      "12% of the rows have been processed at 2023-10-26 14:30:56.161966\n",
      "13% of the rows have been processed at 2023-10-26 14:33:44.860202\n",
      "14% of the rows have been processed at 2023-10-26 14:36:49.727401\n",
      "15% of the rows have been processed at 2023-10-26 14:40:37.847612\n",
      "16% of the rows have been processed at 2023-10-26 14:44:25.106596\n",
      "17% of the rows have been processed at 2023-10-26 14:48:46.792727\n",
      "18% of the rows have been processed at 2023-10-26 14:53:04.896669\n",
      "19% of the rows have been processed at 2023-10-26 14:57:16.277025\n",
      "20% of the rows have been processed at 2023-10-26 15:01:59.548443\n",
      "21% of the rows have been processed at 2023-10-26 15:06:54.931312\n",
      "22% of the rows have been processed at 2023-10-26 15:12:12.870210\n",
      "23% of the rows have been processed at 2023-10-26 15:17:27.374960\n",
      "24% of the rows have been processed at 2023-10-26 15:23:09.317211\n",
      "25% of the rows have been processed at 2023-10-26 15:28:52.240807\n",
      "26% of the rows have been processed at 2023-10-26 15:35:04.564448\n",
      "27% of the rows have been processed at 2023-10-26 15:41:56.604118\n",
      "28% of the rows have been processed at 2023-10-26 15:48:44.393533\n",
      "29% of the rows have been processed at 2023-10-26 15:55:43.235392\n",
      "30% of the rows have been processed at 2023-10-26 16:02:50.749813\n",
      "31% of the rows have been processed at 2023-10-26 16:10:25.726697\n",
      "32% of the rows have been processed at 2023-10-26 16:18:18.016619\n",
      "33% of the rows have been processed at 2023-10-26 16:26:57.702859\n",
      "34% of the rows have been processed at 2023-10-26 16:34:43.453741\n",
      "35% of the rows have been processed at 2023-10-26 16:43:17.987978\n",
      "36% of the rows have been processed at 2023-10-26 16:51:32.693866\n",
      "37% of the rows have been processed at 2023-10-26 17:00:46.775366\n",
      "38% of the rows have been processed at 2023-10-26 17:09:54.495189\n",
      "39% of the rows have been processed at 2023-10-26 17:19:10.329732\n",
      "40% of the rows have been processed at 2023-10-26 17:28:31.304454\n",
      "41% of the rows have been processed at 2023-10-26 17:38:27.118458\n",
      "42% of the rows have been processed at 2023-10-26 17:48:24.028655\n",
      "43% of the rows have been processed at 2023-10-26 17:58:41.853564\n",
      "44% of the rows have been processed at 2023-10-26 18:09:41.491604\n",
      "45% of the rows have been processed at 2023-10-26 18:20:06.477133\n",
      "46% of the rows have been processed at 2023-10-26 18:31:04.308405\n",
      "47% of the rows have been processed at 2023-10-26 18:42:39.843658\n",
      "48% of the rows have been processed at 2023-10-26 18:54:06.480293\n",
      "49% of the rows have been processed at 2023-10-26 19:06:19.691238\n",
      "50% of the rows have been processed at 2023-10-26 19:18:28.230165\n",
      "51% of the rows have been processed at 2023-10-26 19:31:08.399732\n",
      "52% of the rows have been processed at 2023-10-26 19:42:52.497754\n",
      "53% of the rows have been processed at 2023-10-26 21:54:11.315378\n",
      "54% of the rows have been processed at 2023-10-26 22:07:39.500875\n",
      "55% of the rows have been processed at 2023-10-26 22:20:48.778438\n",
      "56% of the rows have been processed at 2023-10-26 22:33:28.466016\n",
      "57% of the rows have been processed at 2023-10-26 22:44:19.554988\n",
      "58% of the rows have been processed at 2023-10-26 22:55:26.260054\n",
      "59% of the rows have been processed at 2023-10-26 23:06:38.020331\n",
      "60% of the rows have been processed at 2023-10-26 23:17:54.663204\n",
      "61% of the rows have been processed at 2023-10-26 23:29:58.921511\n",
      "62% of the rows have been processed at 2023-10-26 23:41:41.813305\n",
      "63% of the rows have been processed at 2023-10-26 23:53:03.412226\n",
      "64% of the rows have been processed at 2023-10-27 00:05:04.558030\n",
      "65% of the rows have been processed at 2023-10-27 00:17:00.844010\n",
      "66% of the rows have been processed at 2023-10-27 00:29:25.272199\n",
      "67% of the rows have been processed at 2023-10-27 00:42:23.908246\n",
      "68% of the rows have been processed at 2023-10-27 00:54:37.284102\n",
      "69% of the rows have been processed at 2023-10-27 01:07:40.214258\n",
      "70% of the rows have been processed at 2023-10-27 01:20:00.059357\n",
      "71% of the rows have been processed at 2023-10-27 01:32:29.323600\n",
      "72% of the rows have been processed at 2023-10-27 01:43:51.625132\n",
      "73% of the rows have been processed at 2023-10-27 01:57:32.042180\n",
      "74% of the rows have been processed at 2023-10-27 02:07:52.445226\n",
      "75% of the rows have been processed at 2023-10-27 02:21:26.439708\n",
      "76% of the rows have been processed at 2023-10-27 02:34:34.443642\n",
      "77% of the rows have been processed at 2023-10-27 02:48:51.184971\n",
      "78% of the rows have been processed at 2023-10-27 03:02:47.667040\n",
      "79% of the rows have been processed at 2023-10-27 03:16:18.562415\n",
      "80% of the rows have been processed at 2023-10-27 03:28:27.807334\n",
      "81% of the rows have been processed at 2023-10-27 03:43:49.781095\n",
      "82% of the rows have been processed at 2023-10-27 04:00:18.578863\n",
      "83% of the rows have been processed at 2023-10-27 04:16:27.719528\n",
      "84% of the rows have been processed at 2023-10-27 04:34:20.057990\n",
      "85% of the rows have been processed at 2023-10-27 04:51:34.414884\n",
      "86% of the rows have been processed at 2023-10-27 05:08:55.089852\n",
      "87% of the rows have been processed at 2023-10-27 05:25:41.730463\n",
      "88% of the rows have been processed at 2023-10-27 05:43:56.528192\n",
      "89% of the rows have been processed at 2023-10-27 06:02:42.571330\n",
      "90% of the rows have been processed at 2023-10-27 06:20:06.822509\n",
      "91% of the rows have been processed at 2023-10-27 06:37:23.741924\n",
      "92% of the rows have been processed at 2023-10-27 06:56:08.813616\n",
      "93% of the rows have been processed at 2023-10-27 07:15:04.064006\n",
      "94% of the rows have been processed at 2023-10-27 07:33:43.087332\n",
      "95% of the rows have been processed at 2023-10-27 08:06:18.536653\n",
      "96% of the rows have been processed at 2023-10-27 08:27:16.021468\n",
      "97% of the rows have been processed at 2023-10-27 09:14:35.921273\n",
      "98% of the rows have been processed at 2023-10-27 09:34:24.105932\n",
      "99% of the rows have been processed at 2023-10-27 09:53:50.216981\n"
     ]
    }
   ],
   "source": [
    "# in this block, I merge all the data to a single file\n",
    "\n",
    "base_dir = \"/Users/alan/Desktop/BI/Thesis/1st_draft/Data/StockTwits\"\n",
    "\n",
    "df = pd.DataFrame(columns=[\"body\",\"sentiment\"])\n",
    "\n",
    "# count the amount of files contained in all the folders\n",
    "count = 0\n",
    "for folder_name in os.listdir(base_dir):\n",
    "    if not folder_name.startswith(\".\") and not folder_name.endswith(\".csv\"):\n",
    "        folder_path = os.path.join(base_dir,folder_name)\n",
    "        ## Here, I just want to concatenate 2 columns which are \"body\" and \"sentiment\"\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path,file_name)\n",
    "            df_sub = pd.read_csv(file_path)\n",
    "            for row in df_sub.iterrows():\n",
    "                count += 1\n",
    "\n",
    "checkpoints = {int(count*perc/100): perc for perc in range(0,100,1)}\n",
    "\n",
    "counter = 0\n",
    "for folder_name in os.listdir(base_dir):\n",
    "    if not folder_name.startswith(\".\") and not folder_name.endswith(\".csv\"):\n",
    "        folder_path = os.path.join(base_dir,folder_name)\n",
    "        ## Here, I just want to concatenate 2 columns which are \"body\" and \"sentiment\"\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path,file_name)\n",
    "            df_sub = pd.read_csv(file_path)\n",
    "            for row in df_sub.iterrows():\n",
    "                if ast.literal_eval(row[1][\"entities\"])[\"sentiment\"] == None:\n",
    "                    counter += 1\n",
    "                    ## I also add the check point so I know the progress\n",
    "                    if counter+1 in checkpoints:\n",
    "                        print(\"{}% of the rows have been processed at {}\".format(checkpoints[counter+1], datetime.utcnow()))\n",
    "                    continue\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    body = row[1][\"body\"]\n",
    "                    sentiment = ast.literal_eval(row[1][\"entities\"])[\"sentiment\"][\"basic\"]\n",
    "                    data = np.array([[body,sentiment]])\n",
    "                    row_new = pd.DataFrame(data, columns=[\"body\",\"sentiment\"])\n",
    "                    df = pd.concat([df, row_new], ignore_index=True)\n",
    "                \n",
    "                if counter+1 in checkpoints:\n",
    "                    print(\"{}% of the rows have been processed at {}\".format(checkpoints[counter+1], datetime.utcnow()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2090191</td>\n",
       "      <td>2090191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2084953</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>$TSLA its time</td>\n",
       "      <td>Bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1570211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  body sentiment\n",
       "count          2090191   2090191\n",
       "unique         2084953         2\n",
       "top     $TSLA its time   Bullish\n",
       "freq                 2   1570211"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we merge the data with the indian stock market data\n",
    "df = pd.read_csv(\"/Users/alan/Desktop/BI/Thesis/1st_draft/Data/merged_data.csv\")\n",
    "df_india = pd.read_csv(\"/Users/alan/Desktop/BI/Thesis/1st_draft/Data/StockTwits/tweets.csv\").iloc[:,1:]\n",
    "df_india.columns = df.columns\n",
    "df_merged = pd.concat([df,df_india],ignore_index=True)\n",
    "df_merged = pd.concat([df,df_india],ignore_index=True)\n",
    "df_merged.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"fine_tune_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this chunk, I will try to collect the stock market data from Tushare\n",
    "ts.set_token(\"82ae03fc12b920fc793249158a6956785a6093caf0fdbae2c4982cd0\")\n",
    "pro = ts.pro_api()\n",
    "\n",
    "df_SH = pro.index_daily(ts_code = \"000001.SH\")\n",
    "df_SH.to_csv(\"/Users/alan/Desktop/BI/Thesis/1st_draft/Data/SH.csv\",index = False)\n",
    "df_SZ = pro.index_daily(ts_code = \"399005.SZ\")\n",
    "df_SZ.to_csv(\"/Users/alan/Desktop/BI/Thesis/1st_draft/Data/SZ.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we are already on 20160417.\n",
      "Now we are already on 20160517.\n",
      "Now we are already on 20160616.\n",
      "Now we are already on 20160716.\n",
      "Now we are already on 20160815.\n",
      "Now we are already on 20160914.\n",
      "Now we are already on 20161014.\n",
      "Now we are already on 20161113.\n",
      "Now we are already on 20161213.\n",
      "Now we are already on 20170112.\n",
      "Now we are already on 20170211.\n",
      "Now we are already on 20170313.\n",
      "Now we are already on 20170412.\n",
      "Now we are already on 20170512.\n",
      "Now we are already on 20170611.\n",
      "Now we are already on 20170711.\n",
      "Now we are already on 20170810.\n",
      "Now we are already on 20170909.\n",
      "Now we are already on 20171009.\n",
      "Now we are already on 20171108.\n",
      "Now we are already on 20171208.\n",
      "Now we are already on 20180107.\n",
      "Now we are already on 20180206.\n",
      "Now we are already on 20180308.\n",
      "Now we are already on 20180407.\n",
      "Now we are already on 20180507.\n",
      "Now we are already on 20180606.\n",
      "Now we are already on 20180706.\n",
      "Now we are already on 20180805.\n",
      "Now we are already on 20180904.\n",
      "Now we are already on 20181004.\n",
      "Now we are already on 20181103.\n",
      "Now we are already on 20181203.\n",
      "Now we are already on 20190102.\n",
      "Now we are already on 20190201.\n",
      "Now we are already on 20190303.\n",
      "Now we are already on 20190402.\n",
      "Now we are already on 20190502.\n",
      "Now we are already on 20190601.\n",
      "Now we are already on 20190701.\n",
      "Now we are already on 20190731.\n",
      "Now we are already on 20190830.\n",
      "Now we are already on 20190929.\n",
      "Now we are already on 20191029.\n",
      "Now we are already on 20191128.\n",
      "Now we are already on 20191228.\n",
      "Now we are already on 20200127.\n",
      "Now we are already on 20200226.\n",
      "Now we are already on 20200327.\n",
      "Now we are already on 20200426.\n",
      "Now we are already on 20200526.\n",
      "Now we are already on 20200625.\n",
      "Now we are already on 20200725.\n",
      "Now we are already on 20200824.\n",
      "Now we are already on 20200923.\n",
      "Now we are already on 20201023.\n",
      "Now we are already on 20201122.\n",
      "Now we are already on 20201222.\n",
      "Now we are already on 20210121.\n",
      "Now we are already on 20210220.\n",
      "Now we are already on 20210322.\n",
      "Now we are already on 20210421.\n",
      "Now we are already on 20210521.\n",
      "Now we are already on 20210620.\n",
      "Now we are already on 20210720.\n",
      "Now we are already on 20210819.\n",
      "Now we are already on 20210918.\n",
      "Now we are already on 20211018.\n",
      "Now we are already on 20211117.\n",
      "Now we are already on 20211217.\n",
      "Now we are already on 20220116.\n",
      "Now we are already on 20220215.\n",
      "Now we are already on 20220317.\n",
      "Now we are already on 20220416.\n",
      "Now we are already on 20220516.\n",
      "Now we are already on 20220615.\n",
      "Now we are already on 20220715.\n",
      "Now we are already on 20220814.\n",
      "Now we are already on 20220913.\n",
      "Now we are already on 20221013.\n",
      "Now we are already on 20221112.\n",
      "Now we are already on 20221212.\n",
      "Now we are already on 20230111.\n",
      "Now we are already on 20230210.\n",
      "Now we are already on 20230312.\n",
      "Now we are already on 20230411.\n",
      "Now we are already on 20230511.\n",
      "Now we are already on 20230610.\n",
      "Now we are already on 20230710.\n",
      "Now we are already on 20230809.\n",
      "Now we are already on 20230908.\n",
      "Now we are already on 20231008.\n"
     ]
    }
   ],
   "source": [
    "# in this chunk, I extract all the available information of the script of CCTV\n",
    "start_date = date(2006,7,1)\n",
    "end_date = date(2023,10,26)\n",
    "current_date = start_date\n",
    "\n",
    "month = 0\n",
    "df_cctvnews = pd.DataFrame(columns = [\"date\",\"title\",\"content\"])\n",
    "while current_date <= end_date:\n",
    "    string = str(current_date)\n",
    "    date_str = string[:4]+string[5:7]+string[8:] \n",
    "    df_news = pro.cctv_news(date = date_str)\n",
    "    df_cctvnews = pd.concat([df_cctvnews,df_news],ignore_index=True)\n",
    "    current_date += timedelta(days = 1)\n",
    "    month += 1\n",
    "    if month % 30 == 0:\n",
    "        print(\"Now we are already on {}.\".format(date_str))\n",
    "        \n",
    "df_cctvnews.to_csv(\"/Users/alan/Desktop/BI/Thesis/1st_draft/Data/cctvnews.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-02-28 23:57:14</td>\n",
       "      <td>【阿联酋修建巨型地下战略油库】阿联酋阿布扎比国家石油公司日前宣布，该公司正在阿联酋富查伊拉酋...</td>\n",
       "      <td>阿联酋修建巨型地下战略油库</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-28 23:55:01</td>\n",
       "      <td>【海航辟谣破产传言：称最艰难时期已过 正全力化解流动性风险】针对近期网传“海航破产”传闻，海...</td>\n",
       "      <td>海航辟谣破产传言：称最艰难时期已过 正全力化解流动性风险</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-02-28 23:48:31</td>\n",
       "      <td>【熊猫金控剥离银湖网生变 股东表决大会被取消】2月28日，熊猫金控股份有限公司(下称“熊猫金...</td>\n",
       "      <td>熊猫金控剥离银湖网生变 股东表决大会被取消</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-02-28 23:37:32</td>\n",
       "      <td>【行情】东方财富网28日讯，在岸人民币兑美元(CNY)北京时间23:30收报6.6935元，...</td>\n",
       "      <td>在岸人民币兑美元(CNY)北京时间23:30收报6.6935元</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-02-28 23:35:24</td>\n",
       "      <td>【美股三大股指窄幅震荡 京东财报靓丽股价涨逾5%】东方财富网28日讯，美东时间周四，美股三大...</td>\n",
       "      <td>美股三大股指窄幅震荡 京东财报靓丽股价涨逾5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2019-02-26 14:13:08</td>\n",
       "      <td>【创业板涨近3%】东方财富网26日讯，创业板涨近3%，两市成交额破9000亿元。</td>\n",
       "      <td>创业板涨近3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2019-02-26 14:10:16</td>\n",
       "      <td>【央行副行长潘功胜：扩大金融服务业双向开放】央行副行长、国家外汇管理局局长潘功胜在接受央视财...</td>\n",
       "      <td>扩大金融服务业双向开放</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2019-02-26 14:07:48</td>\n",
       "      <td>【宁夏出台22条措施促进跨境贸易便利化】记者26日从宁夏回族自治区政府获悉，为优化口岸营商环...</td>\n",
       "      <td>宁夏出22条措施促进跨境贸易便利化</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2019-02-26 14:07:22</td>\n",
       "      <td>【中国铁建再获莫斯科地铁工程合同】莫斯科地铁第三换乘环线东段盾构施工合同签约仪式25日在莫斯...</td>\n",
       "      <td>中铁建再获莫斯科地铁工程合同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2019-02-26 14:06:51</td>\n",
       "      <td>【北京证监局披露京沪高铁首次公开发行股票并在主板上市辅导基本情况表】北京证监局披露京沪高铁首...</td>\n",
       "      <td>北京证监局披露京沪高铁首次公开发行股票并在主板上市辅导基本情况表</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                datetime                                            content  \\\n",
       "0    2019-02-28 23:57:14  【阿联酋修建巨型地下战略油库】阿联酋阿布扎比国家石油公司日前宣布，该公司正在阿联酋富查伊拉酋...   \n",
       "1    2019-02-28 23:55:01  【海航辟谣破产传言：称最艰难时期已过 正全力化解流动性风险】针对近期网传“海航破产”传闻，海...   \n",
       "2    2019-02-28 23:48:31  【熊猫金控剥离银湖网生变 股东表决大会被取消】2月28日，熊猫金控股份有限公司(下称“熊猫金...   \n",
       "3    2019-02-28 23:37:32  【行情】东方财富网28日讯，在岸人民币兑美元(CNY)北京时间23:30收报6.6935元，...   \n",
       "4    2019-02-28 23:35:24  【美股三大股指窄幅震荡 京东财报靓丽股价涨逾5%】东方财富网28日讯，美东时间周四，美股三大...   \n",
       "..                   ...                                                ...   \n",
       "995  2019-02-26 14:13:08           【创业板涨近3%】东方财富网26日讯，创业板涨近3%，两市成交额破9000亿元。   \n",
       "996  2019-02-26 14:10:16  【央行副行长潘功胜：扩大金融服务业双向开放】央行副行长、国家外汇管理局局长潘功胜在接受央视财...   \n",
       "997  2019-02-26 14:07:48  【宁夏出台22条措施促进跨境贸易便利化】记者26日从宁夏回族自治区政府获悉，为优化口岸营商环...   \n",
       "998  2019-02-26 14:07:22  【中国铁建再获莫斯科地铁工程合同】莫斯科地铁第三换乘环线东段盾构施工合同签约仪式25日在莫斯...   \n",
       "999  2019-02-26 14:06:51  【北京证监局披露京沪高铁首次公开发行股票并在主板上市辅导基本情况表】北京证监局披露京沪高铁首...   \n",
       "\n",
       "                                title  \n",
       "0                       阿联酋修建巨型地下战略油库  \n",
       "1        海航辟谣破产传言：称最艰难时期已过 正全力化解流动性风险  \n",
       "2               熊猫金控剥离银湖网生变 股东表决大会被取消  \n",
       "3     在岸人民币兑美元(CNY)北京时间23:30收报6.6935元  \n",
       "4             美股三大股指窄幅震荡 京东财报靓丽股价涨逾5%  \n",
       "..                                ...  \n",
       "995                           创业板涨近3%  \n",
       "996                       扩大金融服务业双向开放  \n",
       "997                 宁夏出22条措施促进跨境贸易便利化  \n",
       "998                    中铁建再获莫斯科地铁工程合同  \n",
       "999  北京证监局披露京沪高铁首次公开发行股票并在主板上市辅导基本情况表  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I ll try the media news\n",
    "df = pro.news(src='eastmoney', start_date=\"2019-01-01\", end_date=\"2019-03-01\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600846f640914a1a814908f78810b96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "model = BertModel.from_pretrained(\"hfl/chinese-bert-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
